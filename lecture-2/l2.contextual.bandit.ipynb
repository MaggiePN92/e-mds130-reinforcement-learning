{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the contextual bandit environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating simulator for the environment\n",
    "class ContextBandit:\n",
    "    def __init__(self, bandits=10):\n",
    "        self.bandits = bandits\n",
    "        self.arms = self.bandits\n",
    "        self.init_distribution()\n",
    "        self.updateState()\n",
    "\n",
    "    def init_distribution(self):\n",
    "        # Setting up different reward distribution over the actions for each state\n",
    "        self.banditMatrix = np.random.rand(self.bandits, self.arms)\n",
    "\n",
    "\n",
    "    def reward(self, prob, n=10):\n",
    "        reward = 0\n",
    "        # Iterates to generate a better sample\n",
    "        for i in range(n):\n",
    "            if np.random.random() < prob:\n",
    "                reward += 1\n",
    "\n",
    "        return reward\n",
    "\n",
    "    # Gets the current state as the website the agent is at\n",
    "    def getState(self):\n",
    "        return self.state\n",
    "\n",
    "    # Updates the current state\n",
    "    def updateState(self):\n",
    "        self.state = np.random.randint(self.bandits)\n",
    "\n",
    "    def getReward(self, arm):\n",
    "        return self.reward(self.banditMatrix[self.getState()][arm])\n",
    "    \n",
    "    # Simulates placing a recommendation and returns its associated reward\n",
    "    def chooseArm(self, arm):\n",
    "        reward = self.getReward(arm)\n",
    "        self.updateState()\n",
    "        return reward\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Set the numpy random seed\n",
    "np.random.seed(42)\n",
    "env = ContextBandit(bandits=10)\n",
    "state = env.getState()\n",
    "reward = env.chooseArm(1)\n",
    "print(state)\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the hyperparameters\n",
    "# Number of bandits. For convenience we will have the number of arms equal to the number of bandits\n",
    "bandits = 10\n",
    "\n",
    "# Specify the batch size. Since we are looking at one state per play, our batch size is one\n",
    "batchSize = 1\n",
    "\n",
    "# The num+er of input neurons will be equal to the number of bandits as we want to recommand on of the websites\n",
    "inNeuron = bandits\n",
    "# The number of hidden neurons are arbitrarily choosen\n",
    "hNeurons = 100\n",
    "# The number of output neurons will be equal to the number of arms as we will recommand one of them\n",
    "oNeurons = bandits \n",
    "\n",
    "# Specify the number of plays / epochs\n",
    "epochs = 5000\n",
    "\n",
    "# Specify the learning rate \n",
    "lr = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple sequential feed-forward NN\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(inNeuron, hNeurons),\n",
    "    torch.nn.ReLU(), # Using RELU activation\n",
    "    torch.nn.Linear(hNeurons, oNeurons),\n",
    "    torch.nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mean Square Error as our loss function\n",
    "lossFun = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ADAM optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also create a helper function to encode a vector where all but 1 element is set to zero. The nonzero element is set to 1 and indicates a purticular state in the state space\n",
    "def oneHot(N, pos, val=1):\n",
    "    oneHotVector = np.zeros(N)\n",
    "    oneHotVector[pos] = val\n",
    "    return oneHotVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need another helper function to calculate the softmax . Since the banditMatrix has different reward probabilities for each action in a state, therefore the softmax will also generate different reward distributions over actions for each state\n",
    "def softmax(av, tau):\n",
    "    softm = np.exp(av /tau) / np.sum( np.exp(av / tau))\n",
    "    return softm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the training loop function\n",
    "def train(env, epochs=epochs, learning_rate=1e-2):\n",
    "    # Creates a 1-order tensor (vector) of size bandits, where all values are zero except the one for current state. The current state will have a value of one\n",
    "    currentState = torch.Tensor(oneHot(bandits, env.getState()))\n",
    "    # Stores the rewards\n",
    "    rewards = []\n",
    "\n",
    "    for e in range(epochs):\n",
    "        yPred = model(currentState)\n",
    "        # Convert reward preditions to probability distribution with softmax\n",
    "        avSoftmax = softmax(yPred.data.numpy(), tau=2.0)        \n",
    "        avSoftmax = avSoftmax / avSoftmax.sum() # Normalize the distribution to ensure that the sum is equal to one\n",
    "        actionChoice = np.random.choice(bandits, p=avSoftmax) # Next action is choosen based on the probability distribution\n",
    "        # Takes action, to receive reward\n",
    "        currentReward = env.chooseArm(actionChoice)\n",
    "        # Copy the pyTorch data to numpy array\n",
    "        oneHotReward = yPred.data.numpy().copy()\n",
    "        # Updates oneHotReward array to use as labeled training data\n",
    "        oneHotReward[actionChoice] = currentReward\n",
    "        reward = torch.Tensor(oneHotReward)\n",
    "        rewards.append(currentReward)\n",
    "        loss = lossFun(yPred, reward)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Updates t he current environment state\n",
    "        currentState = torch.Tensor(oneHot(bandits, env.getState()))\n",
    "    return np.array(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the running average\n",
    "runningRewards = [0]\n",
    "\n",
    "for i, r in enumerate(rewards):\n",
    "    meanReward = ( (i+1) * rewards[-1] + r) / (i+2)\n",
    "    runningRewards.append(meanReward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f8add0801f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us plot the running average rewards over the plays\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "ax.set_xlabel('Plays')\n",
    "ax.set_ylabel('Average Rewards')\n",
    "ax.scatter(np.arange(epochs + 1), runningRewards, s=2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
