{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6],\n",
       "       [7, 8, 9]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an array in numpy\n",
    "\n",
    "import numpy as np\n",
    "np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.],\n",
       "        [7., 8., 9.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a tensor in pytorch\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic differentiation\n",
    "- Numpy natively doesnot offer automatic differentiation and optimization\n",
    "- PyTorch inherently embeds these features into their data structures or tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (torch.Size([7, 3])):\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 0.]])\n",
      "y (torch.Size([7, 1])):\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "weights (torch.Size([1, 3])):\n",
      "tensor([[0.3540, 0.2952, 0.4173]], grad_fn=<PermuteBackward0>)\n",
      "bias (torch.Size([1])):tensor([0.4891], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "numIn = 3\n",
    "numHidden = 1\n",
    "numOut = 1\n",
    "\n",
    "# Example Dataset\n",
    "X = torch.Tensor([[0, 1, 0], [0, 0, 1], [1, 0, 0], [1, 1, 0], [1, 1, 1], [0, 1, 1], [0, 1, 0]])\n",
    "print('X ({}):\\n{}'.format(X.shape, X))\n",
    "y = torch.Tensor([[1], [0], [0], [1], [1], [0], [1]])\n",
    "print('y ({}):\\n{}'.format(y.shape, y))\n",
    "\n",
    "# Generate the random weights\n",
    "weights = torch.rand((numIn, numHidden), requires_grad=True).T\n",
    "print('weights ({}):\\n{}'.format(weights.shape, weights))\n",
    "\n",
    "# Generate the random bias \n",
    "bias = torch.rand(numHidden, requires_grad=True)\n",
    "print('bias ({}):{}'.format(bias.shape, bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = weights*X+bias #linear model\n",
    "loss = (torch.sum(y - yhat))**2 #loss function\n",
    "loss.backward() #calculates gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight:  None\n",
      "bias:  tensor([100.3023])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chakr\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten\\src\\ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "# Print the gradients\n",
    "print('weight: ', weights.grad)\n",
    "print('bias: ', bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building Models\n",
    "- letting tensors hold their gradient, allows PyTorch's **nn** module to automatically utilize it\n",
    "  - for setting up feedforward neural network models\n",
    "  - use the built-in optimization algorithms\n",
    "  - automatically train the networks\n",
    "  - without manual specification of back propogation and gradient decent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a simple model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(numIn, numHidden),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(numHidden, numOut)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, error: 1.42\n",
      "epoch: 2500, error: 0.06\n",
      "epoch: 5000, error: 0.04\n",
      "epoch: 7500, error: 0.03\n",
      "epoch: 10000, error: 0.02\n",
      "epoch: 12500, error: 0.02\n",
      "epoch: 15000, error: 0.02\n",
      "epoch: 17500, error: 0.01\n",
      "epoch: 20000, error: 0.01\n",
      "epoch: 22500, error: 0.01\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "numSteps = 25000\n",
    "for step in range(numSteps):\n",
    "    # Forward Pass\n",
    "    y_pred = model(X)\n",
    "    # Calculate the cost\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Set the grads to None\n",
    "    optimizer.zero_grad()\n",
    "    # Backward Pass\n",
    "    loss.backward()\n",
    "    # Updates the parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the progress\n",
    "    if step % np.round(numSteps/10) == 0:               \n",
    "        print('epoch: {:d}, error: {:.2f}'.format(step, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual:\n",
      " tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "predicted:\n",
      " tensor([[1.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Displaying the results\n",
    "import pandas as pd\n",
    "y_pred = model(X)\n",
    "print('actual:\\n', y)\n",
    "print('predicted:\\n', torch.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result:\n",
      "tensor([[0.],\n",
      "        [1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing on unseen data\n",
    "# Test 1\n",
    "X_test = torch.Tensor([[1, 0, 0], [0,1,0]])\n",
    "result = model(X_test)\n",
    "print('result:\\n{}'.format(torch.round(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
